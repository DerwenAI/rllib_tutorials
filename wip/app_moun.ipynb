{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JD0Eu7JWdVqY"
   },
   "source": [
    "# RLlib Sample Application: MountainCar-v0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example uses [RLlib](https://ray.readthedocs.io/en/latest/rllib.html) to trains a policy with the `MountainCar-v0` environment:\n",
    "\n",
    " - <https://gym.openai.com/envs/MountainCar-v0/>\n",
    "\n",
    "For more background about this problem, see:\n",
    "\n",
    "  - [\"Efficient memory-based learning for robot control\"](https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-209.pdf)  \n",
    "[Andrew William Moore](https://www.cl.cam.ac.uk/~awm22/)  \n",
    "University of Cambridge (1990)\n",
    "  - [\"Solving Mountain Car with Q-Learning\"](https://medium.com/@ts1829/solving-mountain-car-with-q-learning-b77bf71b1de2)  \n",
    "[Tim Sullivan](https://twitter.com/ts_1829)\n",
    "  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's make sure that Ray and RLlib are installed…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ray[rllib]\n",
    "!pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then start Ray…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-06 15:26:21,380\tINFO resource_spec.py:212 -- Starting Ray with 3.52 GiB memory available for workers and up to 1.78 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-07-06 15:26:21,614\tWARNING services.py:923 -- Redis failed to start, retrying now.\n",
      "2020-07-06 15:26:21,898\tINFO services.py:1165 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.1.65',\n",
       " 'raylet_ip_address': '192.168.1.65',\n",
       " 'redis_address': '192.168.1.65:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2020-07-06_15-26-21_364551_84337/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2020-07-06_15-26-21_364551_84337/sockets/raylet',\n",
       " 'webui_url': 'localhost:8265',\n",
       " 'session_dir': '/tmp/ray/session_2020-07-06_15-26-21_364551_84337'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "import ray.rllib.agents.ppo as ppo\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a successful launch, the Ray dashboard will be running on a local port:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard URL: http://localhost:8265\n"
     ]
    }
   ],
   "source": [
    "print(\"Dashboard URL: http://{}\".format(ray.get_webui_url()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open that URL in another tab to view the Ray dashboard as the example runs. We'll also set up a checkpoint location to store the trained policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "CHECKPOINT_ROOT = \"tmp/ppo/moun\"\n",
    "shutil.rmtree(CHECKPOINT_ROOT, ignore_errors=True, onerror=None)\n",
    "\n",
    "ray_results = os.getenv(\"HOME\") + \"/ray_results/\"\n",
    "shutil.rmtree(ray_results, ignore_errors=True, onerror=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll configure to train an RLlib policy with the `MountainCar-v0` environment <https://gym.openai.com/envs/MountainCar-v0/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-06 15:32:27,746\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n",
      "2020-07-06 15:32:33,663\tWARNING worker.py:1047 -- The actor or task with ID ffffffffffffffffaf3894db0100 is pending and cannot currently be scheduled. It requires {CPU: 1.000000} for execution and {CPU: 1.000000} for placement, but this node only has remaining {memory: 3.515625 GiB}, {node:192.168.1.65: 1.000000}, {object_store_memory: 1.220703 GiB}. In total there are 0 pending tasks and 2 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n",
      "2020-07-06 15:32:34,053\tINFO (unknown file):0 -- gc.collect() freed 923 refs in 0.29063460799989116 seconds\n"
     ]
    }
   ],
   "source": [
    "SELECT_ENV = \"MountainCar-v0\"\n",
    "\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config[\"log_level\"] = \"WARN\"\n",
    "\n",
    "config[\"num_workers\"] = 4               # default = 2\n",
    "config[\"train_batch_size\"] = 10000      # default = 4000\n",
    "config[\"sgd_minibatch_size\"] = 256      # default = 128\n",
    "config[\"evaluation_num_episodes\"] = 50  # default = 10\n",
    "\n",
    "agent = ppo.PPOTrainer(config, env=SELECT_ENV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, training runs for `40` iterations. Increase the `N_ITER` setting if you want to see the resulting rewards improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 reward -200.00/-200.00/-200.00 len 200.00 saved tmp/ppo/moun/checkpoint_1/checkpoint-1\n",
      "  2 reward -200.00/-200.00/-200.00 len 200.00 saved tmp/ppo/moun/checkpoint_2/checkpoint-2\n",
      "  3 reward -200.00/-200.00/-200.00 len 200.00 saved tmp/ppo/moun/checkpoint_3/checkpoint-3\n",
      "  4 reward -200.00/-200.00/-200.00 len 200.00 saved tmp/ppo/moun/checkpoint_4/checkpoint-4\n",
      "  5 reward -200.00/-200.00/-200.00 len 200.00 saved tmp/ppo/moun/checkpoint_5/checkpoint-5\n",
      "  6 reward -200.00/-200.00/-200.00 len 200.00 saved tmp/ppo/moun/checkpoint_6/checkpoint-6\n",
      "  7 reward -200.00/-200.00/-200.00 len 200.00 saved tmp/ppo/moun/checkpoint_7/checkpoint-7\n",
      "  8 reward -200.00/-200.00/-200.00 len 200.00 saved tmp/ppo/moun/checkpoint_8/checkpoint-8\n",
      "  9 reward -200.00/-200.00/-200.00 len 200.00 saved tmp/ppo/moun/checkpoint_9/checkpoint-9\n",
      " 10 reward -200.00/-200.00/-200.00 len 200.00 saved tmp/ppo/moun/checkpoint_10/checkpoint-10\n",
      " 11 reward -200.00/-200.00/-200.00 len 200.00 saved tmp/ppo/moun/checkpoint_11/checkpoint-11\n",
      " 12 reward -200.00/-200.00/-200.00 len 200.00 saved tmp/ppo/moun/checkpoint_12/checkpoint-12\n",
      " 13 reward -200.00/-200.00/-200.00 len 200.00 saved tmp/ppo/moun/checkpoint_13/checkpoint-13\n",
      " 14 reward -200.00/-200.00/-200.00 len 200.00 saved tmp/ppo/moun/checkpoint_14/checkpoint-14\n",
      " 15 reward -200.00/-200.00/-200.00 len 200.00 saved tmp/ppo/moun/checkpoint_15/checkpoint-15\n",
      " 16 reward -200.00/-200.00/-200.00 len 200.00 saved tmp/ppo/moun/checkpoint_16/checkpoint-16\n",
      " 17 reward -200.00/-200.00/-200.00 len 200.00 saved tmp/ppo/moun/checkpoint_17/checkpoint-17\n",
      " 18 reward -200.00/-200.00/-200.00 len 200.00 saved tmp/ppo/moun/checkpoint_18/checkpoint-18\n",
      " 19 reward -200.00/-200.00/-200.00 len 200.00 saved tmp/ppo/moun/checkpoint_19/checkpoint-19\n",
      " 20 reward -200.00/-200.00/-200.00 len 200.00 saved tmp/ppo/moun/checkpoint_20/checkpoint-20\n",
      " 21 reward -200.00/-200.00/-200.00 len 200.00 saved tmp/ppo/moun/checkpoint_21/checkpoint-21\n",
      " 22 reward -200.00/-200.00/-200.00 len 200.00 saved tmp/ppo/moun/checkpoint_22/checkpoint-22\n",
      " 23 reward -200.00/-200.00/-200.00 len 200.00 saved tmp/ppo/moun/checkpoint_23/checkpoint-23\n",
      " 24 reward -200.00/-200.00/-200.00 len 200.00 saved tmp/ppo/moun/checkpoint_24/checkpoint-24\n",
      " 25 reward -200.00/-200.00/-200.00 len 200.00 saved tmp/ppo/moun/checkpoint_25/checkpoint-25\n",
      " 26 reward -200.00/-200.00/-200.00 len 200.00 saved tmp/ppo/moun/checkpoint_26/checkpoint-26\n",
      " 27 reward -200.00/-200.00/-200.00 len 200.00 saved tmp/ppo/moun/checkpoint_27/checkpoint-27\n",
      " 28 reward -200.00/-200.00/-200.00 len 200.00 saved tmp/ppo/moun/checkpoint_28/checkpoint-28\n",
      " 29 reward -200.00/-200.00/-200.00 len 200.00 saved tmp/ppo/moun/checkpoint_29/checkpoint-29\n",
      " 30 reward -200.00/-200.00/-200.00 len 200.00 saved tmp/ppo/moun/checkpoint_30/checkpoint-30\n",
      " 31 reward -200.00/-200.00/-200.00 len 200.00 saved tmp/ppo/moun/checkpoint_31/checkpoint-31\n",
      " 32 reward -200.00/-200.00/-200.00 len 200.00 saved tmp/ppo/moun/checkpoint_32/checkpoint-32\n",
      " 33 reward -200.00/-200.00/-200.00 len 200.00 saved tmp/ppo/moun/checkpoint_33/checkpoint-33\n",
      " 34 reward -200.00/-200.00/-200.00 len 200.00 saved tmp/ppo/moun/checkpoint_34/checkpoint-34\n",
      " 35 reward -200.00/-200.00/-200.00 len 200.00 saved tmp/ppo/moun/checkpoint_35/checkpoint-35\n",
      " 36 reward -200.00/-200.00/-200.00 len 200.00 saved tmp/ppo/moun/checkpoint_36/checkpoint-36\n",
      " 37 reward -200.00/-200.00/-200.00 len 200.00 saved tmp/ppo/moun/checkpoint_37/checkpoint-37\n",
      " 38 reward -200.00/-200.00/-200.00 len 200.00 saved tmp/ppo/moun/checkpoint_38/checkpoint-38\n",
      " 39 reward -200.00/-200.00/-200.00 len 200.00 saved tmp/ppo/moun/checkpoint_39/checkpoint-39\n",
      " 40 reward -200.00/-200.00/-200.00 len 200.00 saved tmp/ppo/moun/checkpoint_40/checkpoint-40\n"
     ]
    }
   ],
   "source": [
    "N_ITER = 40\n",
    "s = \"{:3d} reward {:6.2f}/{:6.2f}/{:6.2f} len {:6.2f} saved {}\"\n",
    "\n",
    "for n in range(N_ITER):\n",
    "    result = agent.train()\n",
    "    file_name = agent.save(CHECKPOINT_ROOT)\n",
    "\n",
    "    print(s.format(\n",
    "        n + 1,\n",
    "        result[\"episode_reward_min\"],\n",
    "        result[\"episode_reward_mean\"],\n",
    "        result[\"episode_reward_max\"],\n",
    "        result[\"episode_len_mean\"],\n",
    "        file_name\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gp1LgeCJjGLk"
   },
   "source": [
    "Do the episode rewards increase after multiple iterations?\n",
    "That shows whether the policy is improving.\n",
    "\n",
    "Also, print out the policy and model to see the results of training in detail…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "observations (InputLayer)       [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "fc_1 (Dense)                    (None, 256)          768         observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc_value_1 (Dense)              (None, 256)          768         observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc_2 (Dense)                    (None, 256)          65792       fc_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "fc_value_2 (Dense)              (None, 256)          65792       fc_value_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fc_out (Dense)                  (None, 3)            771         fc_2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "value_out (Dense)               (None, 1)            257         fc_value_2[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 134,148\n",
      "Trainable params: 134,148\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "policy = agent.get_policy()\n",
    "model = policy.model\n",
    "print(model.base_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll use the [`rollout` script](https://ray.readthedocs.io/en/latest/rllib-training.html#evaluating-trained-policies) to evaluate the trained policy.\n",
    "\n",
    "This visualizes the \"car\" agent operating within the simulation: rocking back and forth to gain momentum to overcome the mountain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-06 15:41:38,869\tINFO resource_spec.py:212 -- Starting Ray with 3.47 GiB memory available for workers and up to 1.74 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-07-06 15:41:38,979\tWARNING services.py:923 -- Redis failed to start, retrying now.\n",
      "2020-07-06 15:41:39,208\tWARNING services.py:923 -- Redis failed to start, retrying now.\n",
      "2020-07-06 15:41:39,496\tINFO services.py:1165 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8266\u001b[39m\u001b[22m\n",
      "2020-07-06 15:41:40,642\tINFO trainer.py:585 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "2020-07-06 15:41:40,642\tINFO trainer.py:612 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2020-07-06 15:41:44,819\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n",
      "2020-07-06 15:41:45,007\tINFO trainable.py:423 -- Restored on 192.168.1.65 from checkpoint: tmp/ppo/moun/checkpoint_20/checkpoint-20\n",
      "2020-07-06 15:41:45,007\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 20, '_timesteps_total': None, '_time_total': 201.64923691749573, '_episodes_total': 1040}\n",
      "Episode #0: reward: -200.0\n",
      "Episode #1: reward: -200.0\n",
      "Episode #2: reward: -200.0\n",
      "Episode #3: reward: -200.0\n",
      "Episode #4: reward: -200.0\n",
      "Episode #5: reward: -200.0\n",
      "Episode #6: reward: -200.0\n",
      "Episode #7: reward: -200.0\n",
      "Episode #8: reward: -200.0\n",
      "Episode #9: reward: -200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-06 16:19:38,435\tERROR worker.py:1049 -- listen_error_messages_raylet: Connection closed by server.\n",
      "2020-07-06 16:19:38,429\tERROR import_thread.py:93 -- ImportThread: Connection closed by server.\n",
      "2020-07-06 16:19:38,427\tERROR worker.py:949 -- print_logs: Connection closed by server.\n"
     ]
    }
   ],
   "source": [
    "! rllib rollout \\\n",
    "    tmp/ppo/moun/checkpoint_20/checkpoint-20 \\\n",
    "    --config \"{\\\"env\\\": \\\"MountainCar-v0\\\"}\" \\\n",
    "    --run PPO \\\n",
    "    --steps 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tI9vJ1vU6Mj1"
   },
   "source": [
    "The rollout uses the second saved checkpoint, evaluated through `2000` steps.\n",
    "Modify the path to view other checkpoints.\n",
    "\n",
    "---\n",
    "\n",
    "Finally, launch [TensorBoard](https://ray.readthedocs.io/en/latest/rllib-training.html#getting-started) then follow the instructions (copy/paste the URL it generates) to visualize key metrics from training with RLlib…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "!tensorboard --logdir=$HOME/ray_results"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of rllib_ppo_dqn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
