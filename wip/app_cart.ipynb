{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JD0Eu7JWdVqY"
   },
   "source": [
    "# RLlib Sample Application: CartPole-v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example uses [RLlib](https://ray.readthedocs.io/en/latest/rllib.html) to trains a policy with the `CartPole-v1` environment:\n",
    "\n",
    "  - <https://gym.openai.com/envs/CartPole-v1/> \n",
    "\n",
    "Even though this is a relatively simple and quick example to run, its results can be understood quite visually.\n",
    "\n",
    "For more background about this problem, see:\n",
    "\n",
    "  - [\"Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problem\"](https://ieeexplore.ieee.org/document/6313077)  \n",
    "AG Barto, RS Sutton and CW Anderson  \n",
    "*IEEE Transactions on Systems, Man, and Cybernetics* (1983)\n",
    "  \n",
    "  - [\"Cartpole - Introduction to Reinforcement Learning (DQN - Deep Q-Learning)\"](https://towardsdatascience.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288)  \n",
    "[Greg Surma](https://twitter.com/GSurma)\n",
    "\n",
    "---\n",
    "\n",
    "First, make sure that Ray and RLlib are installed, as well as Gym…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ray[rllib]\n",
    "!pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then start Ray…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-06 13:40:12,792\tINFO resource_spec.py:212 -- Starting Ray with 3.37 GiB memory available for workers and up to 1.7 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-07-06 13:40:13,025\tWARNING services.py:923 -- Redis failed to start, retrying now.\n",
      "2020-07-06 13:40:13,347\tINFO services.py:1165 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.1.65',\n",
       " 'raylet_ip_address': '192.168.1.65',\n",
       " 'redis_address': '192.168.1.65:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2020-07-06_13-40-12_777049_83989/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2020-07-06_13-40-12_777049_83989/sockets/raylet',\n",
       " 'webui_url': 'localhost:8265',\n",
       " 'session_dir': '/tmp/ray/session_2020-07-06_13-40-12_777049_83989'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "import ray.rllib.agents.ppo as ppo\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a successful launch, the Ray dashboard will be running on a local port:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard URL: http://localhost:8265\n"
     ]
    }
   ],
   "source": [
    "print(\"Dashboard URL: http://{}\".format(ray.get_webui_url()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open that URL in another tab to view the Ray dashboard as the example runs. We'll also set up a checkpoint location to store the trained policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "CHECKPOINT_ROOT = \"tmp/ppo/cart\"\n",
    "shutil.rmtree(CHECKPOINT_ROOT, ignore_errors=True, onerror=None)\n",
    "\n",
    "ray_results = os.getenv(\"HOME\") + \"/ray_results/\"\n",
    "shutil.rmtree(ray_results, ignore_errors=True, onerror=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll train an RLlib policy with the `CartPole-v1` environment <https://gym.openai.com/envs/CartPole-v1/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-06 13:40:32,685\tINFO trainer.py:585 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "2020-07-06 13:40:32,685\tINFO trainer.py:612 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2020-07-06 13:40:36,464\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "SELECT_ENV = \"CartPole-v1\"\n",
    "\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config[\"log_level\"] = \"WARN\"\n",
    "\n",
    "agent = ppo.PPOTrainer(config, env=SELECT_ENV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 reward   9.00/ 22.65/ 63.00 len  22.65 saved tmp/ppo/cart/checkpoint_1/checkpoint-1\n",
      "  2 reward  12.00/ 42.72/151.00 len  42.72 saved tmp/ppo/cart/checkpoint_2/checkpoint-2\n",
      "  3 reward  12.00/ 68.17/322.00 len  68.17 saved tmp/ppo/cart/checkpoint_3/checkpoint-3\n",
      "  4 reward  13.00/ 97.87/408.00 len  97.87 saved tmp/ppo/cart/checkpoint_4/checkpoint-4\n",
      "  5 reward  13.00/131.53/500.00 len 131.53 saved tmp/ppo/cart/checkpoint_5/checkpoint-5\n",
      "  6 reward  13.00/165.24/500.00 len 165.24 saved tmp/ppo/cart/checkpoint_6/checkpoint-6\n",
      "  7 reward  13.00/202.48/500.00 len 202.48 saved tmp/ppo/cart/checkpoint_7/checkpoint-7\n",
      "  8 reward  22.00/233.83/500.00 len 233.83 saved tmp/ppo/cart/checkpoint_8/checkpoint-8\n",
      "  9 reward  22.00/271.82/500.00 len 271.82 saved tmp/ppo/cart/checkpoint_9/checkpoint-9\n",
      " 10 reward  22.00/302.99/500.00 len 302.99 saved tmp/ppo/cart/checkpoint_10/checkpoint-10\n",
      " 11 reward  29.00/333.17/500.00 len 333.17 saved tmp/ppo/cart/checkpoint_11/checkpoint-11\n",
      " 12 reward  29.00/363.38/500.00 len 363.38 saved tmp/ppo/cart/checkpoint_12/checkpoint-12\n",
      " 13 reward  50.00/393.42/500.00 len 393.42 saved tmp/ppo/cart/checkpoint_13/checkpoint-13\n",
      " 14 reward  50.00/417.69/500.00 len 417.69 saved tmp/ppo/cart/checkpoint_14/checkpoint-14\n",
      " 15 reward 108.00/445.77/500.00 len 445.77 saved tmp/ppo/cart/checkpoint_15/checkpoint-15\n",
      " 16 reward 108.00/459.22/500.00 len 459.22 saved tmp/ppo/cart/checkpoint_16/checkpoint-16\n",
      " 17 reward 114.00/471.73/500.00 len 471.73 saved tmp/ppo/cart/checkpoint_17/checkpoint-17\n",
      " 18 reward 288.00/476.33/500.00 len 476.33 saved tmp/ppo/cart/checkpoint_18/checkpoint-18\n",
      " 19 reward 288.00/478.15/500.00 len 478.15 saved tmp/ppo/cart/checkpoint_19/checkpoint-19\n",
      " 20 reward 340.00/486.69/500.00 len 486.69 saved tmp/ppo/cart/checkpoint_20/checkpoint-20\n",
      " 21 reward 340.00/490.10/500.00 len 490.10 saved tmp/ppo/cart/checkpoint_21/checkpoint-21\n",
      " 22 reward 340.00/493.85/500.00 len 493.85 saved tmp/ppo/cart/checkpoint_22/checkpoint-22\n",
      " 23 reward 340.00/495.12/500.00 len 495.12 saved tmp/ppo/cart/checkpoint_23/checkpoint-23\n",
      " 24 reward 340.00/495.49/500.00 len 495.49 saved tmp/ppo/cart/checkpoint_24/checkpoint-24\n",
      " 25 reward 340.00/495.49/500.00 len 495.49 saved tmp/ppo/cart/checkpoint_25/checkpoint-25\n",
      " 26 reward 340.00/495.49/500.00 len 495.49 saved tmp/ppo/cart/checkpoint_26/checkpoint-26\n",
      " 27 reward 340.00/495.49/500.00 len 495.49 saved tmp/ppo/cart/checkpoint_27/checkpoint-27\n",
      " 28 reward 340.00/495.49/500.00 len 495.49 saved tmp/ppo/cart/checkpoint_28/checkpoint-28\n",
      " 29 reward 340.00/496.00/500.00 len 496.00 saved tmp/ppo/cart/checkpoint_29/checkpoint-29\n",
      " 30 reward 340.00/496.82/500.00 len 496.82 saved tmp/ppo/cart/checkpoint_30/checkpoint-30\n",
      " 31 reward 441.00/499.41/500.00 len 499.41 saved tmp/ppo/cart/checkpoint_31/checkpoint-31\n",
      " 32 reward 441.00/499.41/500.00 len 499.41 saved tmp/ppo/cart/checkpoint_32/checkpoint-32\n",
      " 33 reward 441.00/499.41/500.00 len 499.41 saved tmp/ppo/cart/checkpoint_33/checkpoint-33\n",
      " 34 reward 500.00/500.00/500.00 len 500.00 saved tmp/ppo/cart/checkpoint_34/checkpoint-34\n",
      " 35 reward 500.00/500.00/500.00 len 500.00 saved tmp/ppo/cart/checkpoint_35/checkpoint-35\n",
      " 36 reward 500.00/500.00/500.00 len 500.00 saved tmp/ppo/cart/checkpoint_36/checkpoint-36\n",
      " 37 reward 500.00/500.00/500.00 len 500.00 saved tmp/ppo/cart/checkpoint_37/checkpoint-37\n",
      " 38 reward 500.00/500.00/500.00 len 500.00 saved tmp/ppo/cart/checkpoint_38/checkpoint-38\n",
      " 39 reward 500.00/500.00/500.00 len 500.00 saved tmp/ppo/cart/checkpoint_39/checkpoint-39\n",
      " 40 reward 500.00/500.00/500.00 len 500.00 saved tmp/ppo/cart/checkpoint_40/checkpoint-40\n"
     ]
    }
   ],
   "source": [
    "N_ITER = 40\n",
    "s = \"{:3d} reward {:6.2f}/{:6.2f}/{:6.2f} len {:6.2f} saved {}\"\n",
    "\n",
    "for n in range(N_ITER):\n",
    "    result = agent.train()\n",
    "    file_name = agent.save(CHECKPOINT_ROOT)\n",
    "\n",
    "    print(s.format(\n",
    "        n + 1,\n",
    "        result[\"episode_reward_min\"],\n",
    "        result[\"episode_reward_mean\"],\n",
    "        result[\"episode_reward_max\"],\n",
    "        result[\"episode_len_mean\"],\n",
    "        file_name\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gp1LgeCJjGLk"
   },
   "source": [
    "Do the episode rewards increase after multiple iterations?\n",
    "That shows how the policy is improving.\n",
    "\n",
    "Also, print out the policy and model to see the results of training in detail…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "observations (InputLayer)       [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "fc_1 (Dense)                    (None, 256)          1280        observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc_value_1 (Dense)              (None, 256)          1280        observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc_2 (Dense)                    (None, 256)          65792       fc_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "fc_value_2 (Dense)              (None, 256)          65792       fc_value_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fc_out (Dense)                  (None, 2)            514         fc_2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "value_out (Dense)               (None, 1)            257         fc_value_2[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 134,915\n",
      "Trainable params: 134,915\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "policy = agent.get_policy()\n",
    "model = policy.model\n",
    "print(model.base_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll use the [`rollout` script](https://ray.readthedocs.io/en/latest/rllib-training.html#evaluating-trained-policies) to evaluate the trained policy.\n",
    "\n",
    "This visualizes the \"cartpole\" agent operating within the simulation: moving the cart left or right to avoid having the pole fall over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-06 13:44:33,438\tINFO resource_spec.py:212 -- Starting Ray with 4.0 GiB memory available for workers and up to 2.02 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-07-06 13:44:33,547\tWARNING services.py:923 -- Redis failed to start, retrying now.\n",
      "2020-07-06 13:44:33,772\tWARNING services.py:923 -- Redis failed to start, retrying now.\n",
      "2020-07-06 13:44:34,053\tINFO services.py:1165 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8266\u001b[39m\u001b[22m\n",
      "2020-07-06 13:44:35,190\tINFO trainer.py:585 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "2020-07-06 13:44:35,190\tINFO trainer.py:612 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2020-07-06 13:44:40,608\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n",
      "2020-07-06 13:44:40,796\tINFO trainable.py:423 -- Restored on 192.168.1.65 from checkpoint: tmp/ppo/cart/checkpoint_40/checkpoint-40\n",
      "2020-07-06 13:44:40,796\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 40, '_timesteps_total': None, '_time_total': 195.11768531799316, '_episodes_total': 621}\n",
      "Episode #0: reward: 500.0\n",
      "Episode #1: reward: 500.0\n",
      "Episode #2: reward: 500.0\n",
      "Episode #3: reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "! rllib rollout \\\n",
    "    tmp/ppo/cart/checkpoint_40/checkpoint-40 \\\n",
    "    --config \"{\\\"env\\\": \\\"CartPole-v1\\\"}\" \\\n",
    "    --run PPO \\\n",
    "    --steps 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tI9vJ1vU6Mj1"
   },
   "source": [
    "The rollout uses the second saved checkpoint, evaluated through `2000` steps.\n",
    "Modify the path to view other checkpoints.\n",
    "\n",
    "---\n",
    "\n",
    "Finally, launch [TensorBoard](https://ray.readthedocs.io/en/latest/rllib-training.html#getting-started) then follow the instructions (copy/paste the URL it generates) to visualize key metrics from training with RLlib…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "!tensorboard --logdir=$HOME/ray_results"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of rllib_ppo_dqn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
